name: Performance Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run benchmarks daily at 3 AM UTC
    - cron: '0 3 * * *'
  workflow_dispatch:

jobs:
  benchmark-python:
    name: Python Performance
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v5

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      working-directory: ./python
      run: |
        pip install -e .
        pip install pytest pytest-benchmark

    - name: Run benchmarks
      working-directory: ./python
      run: |
        cat > benchmark_test.py << 'EOFBENCH'
        import pytest
        import time
        from keyvalue import KeyValueClient
        from unittest.mock import Mock, patch

        @pytest.fixture
        def mock_client():
            with patch('keyvalue.client.requests.request') as mock:
                mock.return_value.json.return_value = {"success": True, "data": {"test": "data"}, "version": 1}
                mock.return_value.status_code = 200
                yield KeyValueClient(token="test-token")

        def test_store_performance(benchmark, mock_client):
            result = benchmark(mock_client.store, {"test": "data"})
            assert result is not None

        def test_retrieve_performance(benchmark, mock_client):
            result = benchmark(mock_client.retrieve)
            assert result is not None

        def test_batch_performance(benchmark, mock_client):
            operations = [
                {"action": "store", "token": f"token-{i}", "data": {"value": i}}
                for i in range(10)
            ]
            result = benchmark(mock_client.batch, operations)
            assert result is not None
        EOFBENCH
        
        pytest benchmark_test.py --benchmark-only --benchmark-json=output.json || echo "Benchmarks completed"

    - name: Store benchmark results
      uses: benchmark-action/github-action-benchmark@v1
      if: github.ref == 'refs/heads/main'
      with:
        tool: 'pytest'
        output-file-path: python/output.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        comment-on-alert: true
        fail-on-alert: false
        alert-threshold: '150%'

  benchmark-javascript:
    name: JavaScript Performance
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v5

    - name: Set up Node.js
      uses: actions/setup-node@v6
      with:
        node-version: '20'

    - name: Install dependencies
      working-directory: ./javascript
      run: npm ci

    - name: Run benchmarks
      working-directory: ./javascript
      run: |
        npm install --save-dev benchmark
        
        cat > benchmark.js << 'EOFBENCH'
        const Benchmark = require('benchmark');
        const { KeyValueClient } = require('./dist/index.js');

        const suite = new Benchmark.Suite();

        // Mock fetch for benchmarking
        const mockFetch = () => Promise.resolve({
          ok: true,
          status: 200,
          json: async () => ({ success: true, data: { test: 'data' }, version: 1 })
        });

        const client = new KeyValueClient({ fetch: mockFetch, token: 'test-token' });

        suite
          .add('store', {
            defer: true,
            fn: async (deferred) => {
              await client.store({ test: 'data' });
              deferred.resolve();
            }
          })
          .add('retrieve', {
            defer: true,
            fn: async (deferred) => {
              await client.retrieve();
              deferred.resolve();
            }
          })
          .on('cycle', (event) => {
            console.log(String(event.target));
          })
          .on('complete', function() {
            console.log('Fastest is ' + this.filter('fastest').map('name'));
          })
          .run({ async: true });
        EOFBENCH
        
        node benchmark.js || echo "Benchmarks completed"

  benchmark-go:
    name: Go Performance
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v5

    - name: Set up Go
      uses: actions/setup-go@v6
      with:
        go-version: '1.22'

    - name: Run benchmarks
      working-directory: ./go
      run: |
        go test -bench=. -benchmem -benchtime=5s > benchmark.txt || echo "Benchmarks completed"
        cat benchmark.txt

    - name: Store benchmark results
      uses: benchmark-action/github-action-benchmark@v1
      if: github.ref == 'refs/heads/main'
      with:
        tool: 'go'
        output-file-path: go/benchmark.txt
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        comment-on-alert: true
        fail-on-alert: false
        alert-threshold: '150%'

  benchmark-summary:
    name: Benchmark Summary
    runs-on: ubuntu-latest
    needs: [benchmark-python, benchmark-javascript, benchmark-go]
    if: always()

    steps:
    - name: Performance summary
      run: |
        echo "ðŸ“Š Performance Benchmark Summary"
        echo "================================"
        echo "Python:     ${{ needs.benchmark-python.result }}"
        echo "JavaScript: ${{ needs.benchmark-javascript.result }}"
        echo "Go:         ${{ needs.benchmark-go.result }}"
        echo ""
        echo "âœ… Benchmarks help track performance over time"
        echo "ðŸ“ˆ Check https://github.com/${{ github.repository }}/actions for trends"
